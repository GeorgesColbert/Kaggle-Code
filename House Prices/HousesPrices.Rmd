---
title: "HousePrice"
author: "Georges Colbert"
date: "4/24/2018"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)




```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)




```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)





```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.








```{}
library(tidyr)
library(dplyr)
library(readr)
library(tree)
library(randomForest)
h.train <- read_csv("Desktop/Projects/Kaggle/House Prices/train.csv")
h.test <- read_csv("Desktop/Projects/Kaggle/House Prices/test.csv")





```

Build an ML algorithm that mimimizes the RMSE. My initial decison is to take an ensemble methods approach, by training both a RF algorithm and a 
MultiRegressional Linear algortihm, then find the average of both predictions as final predicted price.  The method goes as follow:

Data Cleaning

 ML: Random Forest
 
ML : Multiple Linear Regression

Prediction


First task is to figure out the NAs and decide how to deal with them. 

```{}
### check which columns have na values

colnames(h.train) [ apply(h.train, 2, anyNA) ]

### Check the number of NAs per column
sapply(h.train, function(x) sum(is.na(x)))

h.train <- as.data.frame(h.train)




```

Once I've indentified the features with NA values, I initially deal with numerical NA values by replacing them with the Median value of the variable.




```{}


### Give missing Numerical Values the Median value

h.train  <- h.train %>% mutate_at(vars(LotFrontage), ~ifelse(is.na(.), median(., na.rm = TRUE), .))

h.train  <- h.train %>% mutate_at(vars(MasVnrArea), ~ifelse(is.na(.), median(., na.rm = TRUE), .))

h.train  <- h.train %>% mutate_at(vars(GarageYrBlt), ~ifelse(is.na(.), median(., na.rm = TRUE), .))




```


When Comparing the list of Variables with NAs with the data description txt file, I notice that for most variables an NA value is not indicative of a missing value point, but rather a legitmate classfication for the respectivecategorical variable. There are only two categorical variables, MasVnrType and Electrical, with NA values that are not actual classification options of the variables. I decide to remove all observations of MasVnrType and Electrical with missing values.


```{}
#### Delete  observations with missing categorical variables 
h.train <- h.train[complete.cases(h.train$MasVnrType), ]

h.train  <- h.train[complete.cases(h.train$Electrical), ]

```


With every other categorical variable with NA values, the NA observation is replaced with "None".




```{}

##### select the categorical variables 


h.train[is.na(h.train)] <- 'None'


colnames(h.train) [ apply(h.train, 2, anyNA) ]




```


The Random Forest algorithm doesn't work well with categorical names that start with a numerical value, so I alter these names remove the number at the beggining.  I also remove the ID collumn, as I figure it has no input into Housing price. 



```{}
set.seed(1)
library(randomForest)


names(h.train)[names(h.train) == '1stFlrSF'] <- 'FstFlrSF'
names(h.train)[names(h.train) == '2ndFlrSF'] <- 'SndFlrSF'
names(h.train)[names(h.train) == '3SsnPorch'] <- 'ThirSsnPorch'

h.train <- as.data.frame(h.train)


h.train.I <- h.train[,-1]

```


now I'm done with the training, set, I have to clean the test set.



Like the training set, I check for all NA values, and replace the NA numerical values with the median value of the corresponding variable. 
Similar to the training set, I also alter the categorical variables with names that beging with numerical values.

```{}

####### test set ########
### check which columns have na values

colnames(h.test) [ apply(h.test, 2, anyNA) ]

### Check the number of NAs per column
sapply(h.test, function(x) sum(is.na(x)))

### make all na numerical variables into median
h.test[sapply(h.test, is.numeric)] <- lapply(h.test[sapply(h.test, is.numeric)], function(x) {ifelse(is.na(x), median(x, na.rm = TRUE), x )})

#### change the variables to the names from the training set
names(h.test)[names(h.test) == '1stFlrSF'] <- 'FstFlrSF'
names(h.test)[names(h.test) == '2ndFlrSF'] <- 'SndFlrSF'
names(h.test)[names(h.test) == '3SsnPorch'] <- 'ThirSsnPorch'


```


Check which is the most common occurence for categorical variables without "NA"  or "None" as a legitimate label, the missing values in these corresponding variables will be replaced with the most common label.


```{}
table(h.test$MSZoning)

table(h.test$Utilities)

table(h.test$Exterior1st)

table(h.test$Exterior2nd)

table(h.test$MasVnrType)

table(h.test$KitchenQual)


table(h.test$Functional)

table(h.test$SaleType)





```






```{}

h.test$MSZoning[is.na(h.test$MSZoning)] <- "RL"

h.test$Utilities[is.na(h.test$Utilities)] <- "NoSeWa"

h.test$Exterior1st[is.na(h.test$Exterior1st)] <- "VinylSd"

h.test$Exterior2nd[is.na(h.test$Exterior2nd)] <- "Other"


h.test$MasVnrType[is.na(h.test$MasVnrType)] <- "None"

h.test$KitchenQual[is.na(h.test$KitchenQual)] <- "TA"


h.test$Functional[is.na(h.test$Functional)] <- "Typ"

h.test$SaleType[is.na(h.test$SaleType)] <- "WD"



```



Now, similar to the training set, the categorical variables with actual "NA" values are altered to reflect "None". 


```{}
##### change all legit NA observations to None

h.test[is.na(h.test)] <- 'None'


### check which columns have na values

colnames(h.test) [ apply(h.test, 2, anyNA) ]

#remove Id collumn
h.test.I <- h.test[,-1]




```



Machine Learning 


Most ML algorithms prefer to deal with numerical data, so I need to first factorized all the categorical variables in both the training set and the test set. 

```{}
### attach both sets together, factorize and then seperate again, then train rf on training set and predict test set
#remove SalePrice

SalePrice <- h.train.I$SalePrice

h.train.I$SalePrice <- NULL

h.set <- bind_rows(h.train.I, h.test.I)

h.set[sapply(h.set, is.character)] <- lapply(h.set[sapply(h.set, is.character)], 
                                                 as.factor)
h.train.I <- slice(h.set,1:1451)

h.test.I <- slice(h.set, 1452:2910)

h.train.I$SalePrice <- SalePrice


```

Ensemble Methods

An ensemble method is a combination of results from multiple models with the goal of improving  prediction accuracy. For this competition, I decided to rely on a Random Forest model along with a multiple linear regression to increase the accuracy of my predictions.

ML : Random Ferest

```{}

###  train rf on training set
rf.house=randomForest(SalePrice~.,data=h.train.I,mtry=26,importance=TRUE)



#### predict test test
yhat.rf = predict(rf.house,newdata=h.test.I)


```

ML : Multiple Linear Regression

```{}

##### create a multiple linear model 


fit1 <- lm(SalePrice~.,data=h.train.I)

#### predict test set 
yhat.lm = predict(fit1,newdata=h.test.I)




```


Now to combine the two models by finding the average of the two predicted house prices for each individual house.

```{}
yhat.mix <- (yhat.rf+yhat.lm)/2

######## attach prediction to test set

h.test$SalePrice<- yhat.mix


Kaggle.submit <- h.test %>% select("Id", "SalePrice")

write.csv(Kaggle.submit, file = "Desktop/Projects/Kaggle/House Prices/kagglesub4.csv", row.names=FALSE)


```


This model ultimately led to a Root Means Squared Error RMSE of 0.14015, which is a top 42% placement, meaning that there are some more Data Mining to be done if I am aiming for a top 25% placement. I'm contemplating either adding a third estimator, or using lasso for feature selections to remove variables that aren't relevant to predictions. 




